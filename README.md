# Micrograd

In this project, I delved into building a neural network from the ground up. To truly grasp the inner workings, I utilized a video tutorial on Micrograd, a compact autograd engine. Micrograd offers a simplified approach to automatic differentiation, a technique crucial for training neural networks. By building the network myself with Micrograd, I gained a deeper understanding of backpropagation, the algorithm that allows us to efficiently calculate the gradients of a loss function with respect to the network's weights. These gradients are then used to update the weights and improve the network's performance.  Micrograd's small scale (around 50 lines of code) made the concepts more transparent, allowing me to visualize the flow of data and calculations within the network. This hands-on experience with Micrograd significantly enhanced my comprehension of neural networks, gradients, and backpropagation.
